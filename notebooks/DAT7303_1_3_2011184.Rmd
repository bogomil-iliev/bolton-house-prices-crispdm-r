---
title: "DAT303_1_3_2011184"
author: "Bogomil Iliev"
studentID: "2011184"
date: "2025-04-12"
module: "DAT7303 - Data Mining and Machine Learning"
assesment: "Porfolio 3"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r pressure, echo=FALSE}
plot(pressure)
```


## 1. Data Description
## 1.1. Installing and loading of necessary libraries

```{r}
# Install packages for initial data description.
install.packages(c("tidyverse", "summarytools", "skimr", "GGally", "ggcorrplot", "scales", "ggplot2", "tidyverse"))


# Load necessary libraries
library(tidyverse)     # data manipulation and visualization
library(summarytools)  # descriptive statistics
library(skimr)         # detailed data summaries
library(ggplot2)
library(scales)        # for comma() formatting
library(ggcorrplot)    # to generate correlational heatmaps.

```

## 1.2. Loading of the Bolton Housing Prices Dataset from 2023.
```{r}

#PLEASE ENSURE YOU CHANGE THE DIRECT PATH WITH THE ONE YOU ARE USING WHEN TESTING THE CODE!!!

#Get the current working directory.
getwd()

#Setting a new working directory
setwd("C:\\Users\\bogoi\\Desktop\\(MSc.) AI\\DAT7303 - Data Mining and Machine Learning\\Assignments\\Portfolio 3\\")

#Double-check the working directory has been changed.
getwd()

#Read the Bolton Housing Prices Dataset from 2023
housing_data <- read.csv('Housing_Data_Same_Region.csv')


```

## 1.3. Examination of the data and its surface properties (Summary Statistics).
```{r}
# View the first 10 rows of the dataset.
head(housing_data, n=10)

#Examining the last 10 records from the dataset. 
tail(housing_data, n=10)

# Get structure of dataset
str(housing_data)

# Summary statistics
summary(housing_data)

# Detailed descriptive statistics using summarytools
view(dfSummary(housing_data))
```

## 1.4. Data Exploration (Preliminary Analysis)
```{r}
#Histogram to show Distribution of Target Variable (SALE_PRICE)

ggplot(housing_data, aes(x = SALE_PRC)) +
  geom_histogram(binwidth = 50000,            # 50‑k bins
                 fill = "steelblue",
                 colour = "black") +
  scale_x_continuous(
    breaks  = seq(0,
                  max(housing_data$SALE_PRC, na.rm = TRUE),
                  by = 250000),               # tick every 250‑k
    labels  = label_number(scale  = 1e-3,     # divide by 1 000
                           suffix = "k",
                           accuracy = 1)      # round to nearest k
  ) +
  labs(title = "Distribution of Sale Prices",
       x     = "Sale Price (USD, thousands)",
       y     = "Frequency") +
  theme_minimal()


#Boxplots to identify outliers

ggplot(housing_data, aes(y = SALE_PRC)) +
  geom_boxplot(fill = "lightblue") +
  scale_y_continuous(
    labels = label_number(scale  = 1e-3,
                          suffix = "k",
                          accuracy = 1)
  ) +
  labs(title = "Boxplot of Sale Prices",
       y     = "Sale Price (USD, thousands)") +
  theme_minimal()


#Generating a correlational heatmap to identify relationships between features.
#Selecting the numeric predictors. 
numeric_data <- housing_data %>%         
  select(
    SALE_PRC, LND_SQFOOT, TOT_LVG_AREA, SPEC_FEAT_VAL,
    RAIL_DIST, OCEAN_DIST, WATER_DIST,
    CNTR_DIST, SUBCNTR_DI, HWY_DIST,
    age, structure_quality
  )

#Create the correlation matrix 
corr_mat <- cor(numeric_data, use = "complete.obs")   

#Draw the heat‑map
ggcorrplot(
  corr_mat,
  method   = "square",
  type     = "lower",              # lower‑triangle only
  hc.order = TRUE,                 # cluster similar vars
  lab      = TRUE,                 # print r‑values
  colors   = c("firebrick", "white", "steelblue2"),
  lab_size = 3
) +
  labs(title = "Correlation Heatmap of Housing Features")

```


## 1.5. Verification of Data Quality.
```{r}
# Check the number of missing values per column
colSums(is.na(housing_data))

#Check for Duplicates by the unique occurances of the PARCEL Numbers.
housing_data %>%
  count(PARCELNO) %>%
  filter(n > 1)

#Check for unusual or out-of-range values.
summary(housing_data$age)
summary(housing_data$structure_quality)

#Visually check for potential anomalies.
housing_data %>%
  filter(age < 0 | structure_quality < 1 | structure_quality > 10)

```



## 2. Data Preparation (Data Munging)

```{r}
#2.1. Install and load libraries needed for the data preparation.

# Install packages.
install.packages(c("janitor", "caret"))

library(janitor)     # clean_names(), duplicate handling
library(caret)       # preprocessing (scaling, centring, Box–Cox)


#2.2. Selecting the data.

#Removing the Latitude, Longitude, ParcelNo columns as irrelevant to the scope of the project and its goals.
housing_pre <- housing_data %>% 
  select(-c(LATITUDE, LONGITUDE, PARCELNO))


#2.3.Cleaning the data.
#Verifying the absence of missing values
stopifnot(sum(is.na(housing_pre)) == 0)   # Should be TRUE


#2.4. Construct data (feature engineering) - keeping both raw and log targets to better aim different models in the modelling phase.


housing_pre <- housing_pre %>%
  mutate(
    # 1. Create log‑transformed sale price (helps linear / SVR models)
    log_SALE_PRC = log10(SALE_PRC),

    # 2. Re‑encode integer codes as factors
    month_sold = factor(month_sold,
                        levels = 1:12,
                        labels = month.abb,
                        ordered = TRUE),
    avno60plus = factor(avno60plus, labels = c("No", "Yes")),
    structure_quality = factor(structure_quality, ordered = TRUE)
  )



#2.5. Integrate the dataset (formatting it for modelling).

#Identify numeric predictors
num_vars <- housing_pre %>% 
  select(where(is.numeric)) %>% 
  names()

# Drop both target columns from scaling list
num_vars <- setdiff(num_vars, c("SALE_PRC", "log_SALE_PRC"))

#Standardise (mean‑centre & unit‑variance) numeric columns
pp <- preProcess(housing_pre[, num_vars], method = c("center", "scale"))
housing_model <- bind_cols(
  predict(pp, housing_pre[, num_vars]),
  housing_pre %>% select(-all_of(num_vars)) #retain raw targets
)

#Final glimpse
glimpse(housing_model)


```



##3. Modelling Phase:
```{r}
#3.1 Installing and loading of neccessary packages for model training.

# Install packages.
install.packages(c("e1071", "randomForest", "rpart","Metrics"))
#install.packages("randomForest") 
install.packages("caret", dependencies = TRUE)
install.packages("doParallel", dependencies = TRUE)
install.packages("LiblineaR")


#Loading of libraries
library(e1071)           # SVM back‑end
library(randomForest)
library(rpart)
library(Metrics)         # rmse, mae
library(doParallel)
library(LiblineaR)
```

```{r}
#3.2. Data Split. The "housing_model" already contains both the raw target and the log transformed one.
#The train-test split is set to 80/20

set.seed(42)
idx   <- createDataPartition(housing_model$SALE_PRC, p = 0.80, list = FALSE)
train <- housing_model[idx,  ]
test  <- housing_model[-idx, ]

#Parallel backend (uses all except one core)

cores <- parallel::detectCores() - 1
cl <- makePSOCKcluster(cores)
registerDoParallel(cl)

#Common re-sampling + verbosity
ctrl <- trainControl(
  method      = "cv",
  number      = 5,          # reduce folds from 10 to 5  (reduces run time by 50%)
  verboseIter = TRUE,
  allowParallel = TRUE
)
```
y

```{r}
# 3.3. Model Training
#?train
# 3.3.1. Multiple Linear Regression (log target).

system.time(
  lm_fit <- train(
    log_SALE_PRC ~ . -SALE_PRC, data = train,
    method = "lm",
    trControl = ctrl)
)
```

```{r}
# 3.3.2. SVR – linear with fast linear back end to speed up training process

lin_grid <- expand.grid(cost = c(0.25, 1, 4, 16))   # 4 C values
system.time(
  svr_lin_fit <- train(
    log_SALE_PRC ~ . -SALE_PRC, data = train,
    method    = "svmLinear2",   # Liblinear
    tuneGrid  = lin_grid,
    trControl = ctrl)
)
```

```{r}
# 3.3.3.  SVR – polynomial kernel
poly_grid <- expand.grid(degree = 2:3, scale = 0.01, C = 2^(0:4))
system.time(
  svr_poly_fit <- train(
    log_SALE_PRC ~ . -SALE_PRC, data = train,
    method    = "svmPoly",
    tuneGrid  = poly_grid,
    trControl = ctrl)
)
```

```{r}
# 3.3.4  SVR – radial (RBF) kernel
system.time(
  svr_rbf_fit <- train(
    log_SALE_PRC ~ . -SALE_PRC, data = train,
    method      = "svmRadial",
    tuneLength  = 8,            # default 10 changed to  8 to reduce run time
    trControl   = ctrl)
)
```
```{r}
# 3.3.5  Decision Tree (raw target)
system.time(
  dt_fit <- train(
    SALE_PRC ~ . -log_SALE_PRC, data = train,
    method      = "rpart",
    tuneLength  = 20,
    trControl   = ctrl)
)
```
```{r}
# 3.3.6  Random Forest (raw target) with live tree count
system.time(
  rf_fit <- train(
    SALE_PRC ~ . -log_SALE_PRC, data = train,
    method    = "rf",
    ntree     = 500,
    do.trace  = 50,        # prints progress every 50 trees
    trControl = ctrl)
)
```
```{r}
# 3.4. Stopping of parallel clustering.
stopCluster(cl)
registerDoSEQ()
```

```{r}
#3.5. Test Set evaluation.

eval_model <- function(name, fit, df, target_raw, log_based = FALSE) {
  preds <- predict(fit, newdata = df)
  if (log_based) preds <- 10^preds
  tibble(Model = name,
         RMSE  = rmse(df[[target_raw]], preds),
         MAE   = mae (df[[target_raw]], preds),
         R2    = R2  (preds, df[[target_raw]]))
}

results <- bind_rows(
  eval_model("Linear (log)"      , lm_fit      , test, "SALE_PRC", TRUE ),
  eval_model("SVR‑Linear (log)"  , svr_lin_fit , test, "SALE_PRC", TRUE ),
  eval_model("SVR‑Poly  (log)"   , svr_poly_fit, test, "SALE_PRC", TRUE ),
  eval_model("SVR‑RBF   (log)"   , svr_rbf_fit , test, "SALE_PRC", TRUE ),
  eval_model("Decision Tree"     , dt_fit      , test, "SALE_PRC", FALSE),
  eval_model("Random Forest"     , rf_fit      , test, "SALE_PRC", FALSE)
)
print(results)

```

```{r}

# 3.6.  RMSE BAR‑CHART

ggplot(results, aes(x = reorder(Model, RMSE), y = RMSE)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  geom_text(aes(label = round(RMSE, 0)), hjust = -0.1, size = 3) +
  labs(title = "Test Set RMSE by Model",
       x = NULL, y = "RMSE (USD)") +
  theme_minimal()
```

## 4. Prediction Generation for a Specified Property.

```{r}

#Inputting the data for prediction as per the assignment brief
new_house <- tibble(
  LND_SQFOOT        = 11247,
  TOT_LVG_AREA      = 4552,
  SPEC_FEAT_VAL     = 2105,
  RAIL_DIST         = 4871.9,
  OCEAN_DIST        = 18507.2,
  WATER_DIST        = 375.8,
  CNTR_DIST         = 43897.9,
  SUBCNTR_DI        = 40115.7,
  HWY_DIST          = 41917.1,
  age               = 42,
  avno60plus        = factor(0, levels = c(0, 1), labels = c("No", "Yes")),
  structure_quality = factor(5, ordered = TRUE),
  month_sold        = factor(8, levels = 1:12, labels = month.abb, ordered = TRUE)
)

#Apply same numeric scaling as the preprocessing in Task 2.
new_num   <- predict(pp, newdata = new_house[, names(pp$mean)])   # pp from Task2
new_ready <- bind_cols(new_num, new_house %>% select(-names(pp$mean)))

#Add dummy column required by rf_fit 
new_ready$log_SALE_PRC <- 0    # placeholder; not used by any split

#Make prediction
pred_price <- predict(rf_fit, newdata = new_ready)   # rf_fit from Task3
round(pred_price, 0)

```

